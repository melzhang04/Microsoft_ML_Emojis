{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08225e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os \n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, Trainer, default_data_collator\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "#pip install einops\n",
    "#package required for downloading transformer \n",
    "#https://huggingface.co/datasets/cnn_dailymail link to dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d47c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cnn_dailymail\",'3.0.0', split='train')\n",
    "#Run loading script once located in same directory as python notebook\n",
    "#Specify configuration version '3.0.0'\n",
    "#load dataset using script because dataset is very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18789234",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(dataset)\n",
    "#cast dataset into pandas dataframe object type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bdcda9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LONDON, England (Reuters) -- Harry Potter star...</td>\n",
       "      <td>Harry Potter star Daniel Radcliffe gets £20M f...</td>\n",
       "      <td>42c027e4ff9730fbb3de84c1af0d2c506e41c3e4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Editor's note: In our Behind the Scenes series...</td>\n",
       "      <td>Mentally ill inmates in Miami are housed on th...</td>\n",
       "      <td>ee8871b15c50d0db17b0179a6d2beab35065f1e9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...</td>\n",
       "      <td>NEW: \"I thought I was going to die,\" driver sa...</td>\n",
       "      <td>06352019a19ae31e527f37f7571c6dd7f0c5da37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WASHINGTON (CNN) -- Doctors removed five small...</td>\n",
       "      <td>Five small polyps found during procedure; \"non...</td>\n",
       "      <td>24521a2abb2e1f5e34e6824e0f9e56904a2b0e88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(CNN)  -- The National Football League has ind...</td>\n",
       "      <td>NEW: NFL chief, Atlanta Falcons owner critical...</td>\n",
       "      <td>7fe70cc8b12fab2d0a258fababf7d9c6b5e1262a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BAGHDAD, Iraq (CNN) -- Dressed in a Superman s...</td>\n",
       "      <td>Parents beam with pride, can't stop from smili...</td>\n",
       "      <td>a1ebb8bb4d370a1fdf28769206d572be60642d70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BAGHDAD, Iraq (CNN) -- The women are too afrai...</td>\n",
       "      <td>Aid workers: Violence, increased cost of livin...</td>\n",
       "      <td>7c0e61ac829a3b3b653e2e3e7536cc4881d1f264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BOGOTA, Colombia (CNN) -- A key rebel commande...</td>\n",
       "      <td>Tomas Medina Caracas was a fugitive from a U.S...</td>\n",
       "      <td>f0d73bdab711763e745cdc75850861c9018f235d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WASHINGTON (CNN) -- White House press secretar...</td>\n",
       "      <td>President Bush says Tony Snow \"will battle can...</td>\n",
       "      <td>5e22bbfc7232418b8d2dd646b952e404df5bd048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(CNN) -- Police and FBI agents are investigati...</td>\n",
       "      <td>Empty anti-tank weapon turns up in front of Ne...</td>\n",
       "      <td>613d6311ec2c1985bd44707d1796d275452fe156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  LONDON, England (Reuters) -- Harry Potter star...   \n",
       "1  Editor's note: In our Behind the Scenes series...   \n",
       "2  MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...   \n",
       "3  WASHINGTON (CNN) -- Doctors removed five small...   \n",
       "4  (CNN)  -- The National Football League has ind...   \n",
       "5  BAGHDAD, Iraq (CNN) -- Dressed in a Superman s...   \n",
       "6  BAGHDAD, Iraq (CNN) -- The women are too afrai...   \n",
       "7  BOGOTA, Colombia (CNN) -- A key rebel commande...   \n",
       "8  WASHINGTON (CNN) -- White House press secretar...   \n",
       "9  (CNN) -- Police and FBI agents are investigati...   \n",
       "\n",
       "                                          highlights  \\\n",
       "0  Harry Potter star Daniel Radcliffe gets £20M f...   \n",
       "1  Mentally ill inmates in Miami are housed on th...   \n",
       "2  NEW: \"I thought I was going to die,\" driver sa...   \n",
       "3  Five small polyps found during procedure; \"non...   \n",
       "4  NEW: NFL chief, Atlanta Falcons owner critical...   \n",
       "5  Parents beam with pride, can't stop from smili...   \n",
       "6  Aid workers: Violence, increased cost of livin...   \n",
       "7  Tomas Medina Caracas was a fugitive from a U.S...   \n",
       "8  President Bush says Tony Snow \"will battle can...   \n",
       "9  Empty anti-tank weapon turns up in front of Ne...   \n",
       "\n",
       "                                         id  \n",
       "0  42c027e4ff9730fbb3de84c1af0d2c506e41c3e4  \n",
       "1  ee8871b15c50d0db17b0179a6d2beab35065f1e9  \n",
       "2  06352019a19ae31e527f37f7571c6dd7f0c5da37  \n",
       "3  24521a2abb2e1f5e34e6824e0f9e56904a2b0e88  \n",
       "4  7fe70cc8b12fab2d0a258fababf7d9c6b5e1262a  \n",
       "5  a1ebb8bb4d370a1fdf28769206d572be60642d70  \n",
       "6  7c0e61ac829a3b3b653e2e3e7536cc4881d1f264  \n",
       "7  f0d73bdab711763e745cdc75850861c9018f235d  \n",
       "8  5e22bbfc7232418b8d2dd646b952e404df5bd048  \n",
       "9  613d6311ec2c1985bd44707d1796d275452fe156  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f6f55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['article','id'], inplace=True)\n",
    "#drop article colummn from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30b74414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry Potter star Daniel Radcliffe gets £20M f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mentally ill inmates in Miami are housed on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEW: \"I thought I was going to die,\" driver sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Five small polyps found during procedure; \"non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEW: NFL chief, Atlanta Falcons owner critical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Parents beam with pride, can't stop from smili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Aid workers: Violence, increased cost of livin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tomas Medina Caracas was a fugitive from a U.S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>President Bush says Tony Snow \"will battle can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Empty anti-tank weapon turns up in front of Ne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          highlights\n",
       "0  Harry Potter star Daniel Radcliffe gets £20M f...\n",
       "1  Mentally ill inmates in Miami are housed on th...\n",
       "2  NEW: \"I thought I was going to die,\" driver sa...\n",
       "3  Five small polyps found during procedure; \"non...\n",
       "4  NEW: NFL chief, Atlanta Falcons owner critical...\n",
       "5  Parents beam with pride, can't stop from smili...\n",
       "6  Aid workers: Violence, increased cost of livin...\n",
       "7  Tomas Medina Caracas was a fugitive from a U.S...\n",
       "8  President Bush says Tony Snow \"will battle can...\n",
       "9  Empty anti-tank weapon turns up in front of Ne..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc48fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download tokenizer associated with \"microsoft/phi-1_5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\",  from_tf=False,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97a22c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "943baced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>highlights</th>\n",
       "      <th>highlights_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry Potter star Daniel Radcliffe gets £20M f...</td>\n",
       "      <td>[Harry, ĠPotter, Ġstar, ĠDaniel, ĠRad, cliffe,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          highlights  \\\n",
       "0  Harry Potter star Daniel Radcliffe gets £20M f...   \n",
       "\n",
       "                                highlights_tokenized  \n",
       "0  [Harry, ĠPotter, Ġstar, ĠDaniel, ĠRad, cliffe,...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take 1000 examples from our dataframe to use for training\n",
    "pretask_dataset=df.iloc[0:1000].copy()\n",
    "\n",
    "#tokenize pretask_dataset 'article' column and turn words into subword tokens for 'article' column stored in new \n",
    "#column name 'article_tokenized'\n",
    "\n",
    "#apply across article column of pretask_dataset tokenizer and store results for each example in corresponing row and new column\n",
    "#name 'article_tokenized'\n",
    "#pretask_dataset['article_'+'tokenized']=pretask_dataset['article'].apply(lambda x: tokenizer.tokenize(x, truncation=True))\n",
    "\n",
    "#same procedure except new column name 'highlights_tokenized'\n",
    "pretask_dataset['highlights_'+'tokenized']=pretask_dataset['highlights'].apply(lambda x: tokenizer.tokenize(x, truncation=True))\n",
    "\n",
    "\n",
    "#peak at pretask_dataset\n",
    "pretask_dataset.head(1)\n",
    "\n",
    "#must use 'lambda x: ' function with tokenizer.tokenize(x) to apply tokenization to all rows \n",
    "#no error was outputted\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "281a2a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>highlights_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Harry', 'ĠPotter', 'Ġstar', 'ĠDaniel', 'ĠRad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['M', 'ent', 'ally', 'Ġill', 'Ġinmates', 'Ġin'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['NEW', ':', 'Ġ\"', 'I', 'Ġthought', 'ĠI', 'Ġwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Five', 'Ġsmall', 'Ġpoly', 'ps', 'Ġfound', 'Ġ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['NEW', ':', 'ĠNFL', 'Ġchief', ',', 'ĠAtlanta'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>['Mother', 'Ġof', 'Ġmurdered', 'Ġschool', 'boy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>['NEW', ':', 'ĠPope', 'ĠBenedict', 'ĠXVI', 'Ġa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>['Eight', 'ĠFlorida', 'Ġteens', 'Ġto', 'Ġbe', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>['Judge', 'Ġon', 'ĠHeather', 'ĠMills', ':', 'Ġ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>['President', 'ĠHarding', \"'s\", 'Ġillegitimate...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  highlights_tokenized\n",
       "0    ['Harry', 'ĠPotter', 'Ġstar', 'ĠDaniel', 'ĠRad...\n",
       "1    ['M', 'ent', 'ally', 'Ġill', 'Ġinmates', 'Ġin'...\n",
       "2    ['NEW', ':', 'Ġ\"', 'I', 'Ġthought', 'ĠI', 'Ġwa...\n",
       "3    ['Five', 'Ġsmall', 'Ġpoly', 'ps', 'Ġfound', 'Ġ...\n",
       "4    ['NEW', ':', 'ĠNFL', 'Ġchief', ',', 'ĠAtlanta'...\n",
       "..                                                 ...\n",
       "995  ['Mother', 'Ġof', 'Ġmurdered', 'Ġschool', 'boy...\n",
       "996  ['NEW', ':', 'ĠPope', 'ĠBenedict', 'ĠXVI', 'Ġa...\n",
       "997  ['Eight', 'ĠFlorida', 'Ġteens', 'Ġto', 'Ġbe', ...\n",
       "998  ['Judge', 'Ġon', 'ĠHeather', 'ĠMills', ':', 'Ġ...\n",
       "999  ['President', 'ĠHarding', \"'s\", 'Ġillegitimate...\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create new dataframe with only tokenized data of article and highlights \n",
    "\n",
    "columns_to_remove= ['highlights']\n",
    "pretask_dataset_tokenized = pretask_dataset.drop(columns=columns_to_remove, axis=1, inplace=False).astype(str)\n",
    "pretask_dataset_tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ec609c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['funny, laughter, joke, comedian', 'sad, upset, crying, hurt', 'happy, joy, smile', '\\xa0thinking, question, learning, curious, mystery student, learning, school', 'celebration, party, birthday', 'scared, worried, concerned', 'angry, mad, upset, hatred, frustrated', 'shocked, OMG, surprise, exasperated', '\\xa0computer, technology, social media', 'money, job, stocks', 'sleepy, tired, sleep, bed, night', 'gross, disgusting, throw up, puke, queasy', '\\xa0dead, deathly, poison, fossil, afterlife', '\\xa0good, great, ok, agreed', 'bad, no, disagree', '\\xa0empowerment, movement', 'teamwork, agreement', '\\xa0workout, sports, athlete, gym, muscle', 'religion, prayer, praise, beg', 'beauty, cosmetics, fashion', '\\xa0baby, kid, child, young', '\\xa0man', 'woman', ' elderly, old', 'love, relationship, significant other, marriage', 'construction, building', 'ocean, sea, ship, water', 'justice, law, government, politics', 'pet, animal, vet', 'nature, plants, farming, science, earth, recycle, go green', 'music, musician, instruments, artist, song', '\\xa0weather, rain, water', 'food, dessert, snack, chef, kitchen', 'drinks, alcohol, party', 'space, explore, astronaut, launch, flight, travel, plane, pilot, sky', 'winning, award, achievement', 'entertainment, acting, movie', 'hospital, sickness, injury, doctor, medicine, emergency', 'fire, firefighters, 911', 'home, house, living, family, domestic']\n"
     ]
    }
   ],
   "source": [
    "#Now work on creating pretask labels from keywords to align with \"keyword presence\" task\n",
    "\n",
    "df_2=pd.read_csv('keywords.csv')\n",
    "\n",
    "#make empty list\n",
    "list1 = []\n",
    "\n",
    "for word in df_2:\n",
    "    for j in df_2[word]:\n",
    "        list1.append(j)\n",
    "\n",
    "print(list1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8758a33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['funny, laughter, joke, comedian', 'sad, upset, crying, hurt', 'happy, joy, smile', 'thinking, question, learning, curious, mystery student, learning, school', 'celebration, party, birthday', 'scared, worried, concerned', 'angry, mad, upset, hatred, frustrated', 'shocked, OMG, surprise, exasperated', 'computer, technology, social media', 'money, job, stocks', 'sleepy, tired, sleep, bed, night', 'gross, disgusting, throw up, puke, queasy', 'dead, deathly, poison, fossil, afterlife', 'good, great, ok, agreed', 'bad, no, disagree', 'empowerment, movement', 'teamwork, agreement', 'workout, sports, athlete, gym, muscle', 'religion, prayer, praise, beg', 'beauty, cosmetics, fashion', 'baby, kid, child, young', 'man', 'woman', ' elderly, old', 'love, relationship, significant other, marriage', 'construction, building', 'ocean, sea, ship, water', 'justice, law, government, politics', 'pet, animal, vet', 'nature, plants, farming, science, earth, recycle, go green', 'music, musician, instruments, artist, song', 'weather, rain, water', 'food, dessert, snack, chef, kitchen', 'drinks, alcohol, party', 'space, explore, astronaut, launch, flight, travel, plane, pilot, sky', 'winning, award, achievement', 'entertainment, acting, movie', 'hospital, sickness, injury, doctor, medicine, emergency', 'fire, firefighters, 911', 'home, house, living, family, domestic']\n"
     ]
    }
   ],
   "source": [
    "characters_to_remove = '\\xa0'\n",
    "\n",
    "cleaned_list = [''.join(char for char in string if char not in characters_to_remove)for string in list1]\n",
    "\n",
    "print(cleaned_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98d2e9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['funny,', 'laughter,', 'joke,', 'comedian', 'sad,', 'upset,', 'crying,', 'hurt', 'happy,', 'joy,', 'smile', 'thinking,', 'question,', 'learning,', 'curious,', 'mystery', 'student,', 'learning,', 'school', 'celebration,', 'party,', 'birthday', 'scared,', 'worried,', 'concerned', 'angry,', 'mad,', 'upset,', 'hatred,', 'frustrated', 'shocked,', 'OMG,', 'surprise,', 'exasperated', 'computer,', 'technology,', 'social', 'media', 'money,', 'job,', 'stocks', 'sleepy,', 'tired,', 'sleep,', 'bed,', 'night', 'gross,', 'disgusting,', 'throw', 'up,', 'puke,', 'queasy', 'dead,', 'deathly,', 'poison,', 'fossil,', 'afterlife', 'good,', 'great,', 'ok,', 'agreed', 'bad,', 'no,', 'disagree', 'empowerment,', 'movement', 'teamwork,', 'agreement', 'workout,', 'sports,', 'athlete,', 'gym,', 'muscle', 'religion,', 'prayer,', 'praise,', 'beg', 'beauty,', 'cosmetics,', 'fashion', 'baby,', 'kid,', 'child,', 'young', 'man', 'woman', 'elderly,', 'old', 'love,', 'relationship,', 'significant', 'other,', 'marriage', 'construction,', 'building', 'ocean,', 'sea,', 'ship,', 'water', 'justice,', 'law,', 'government,', 'politics', 'pet,', 'animal,', 'vet', 'nature,', 'plants,', 'farming,', 'science,', 'earth,', 'recycle,', 'go', 'green', 'music,', 'musician,', 'instruments,', 'artist,', 'song', 'weather,', 'rain,', 'water', 'food,', 'dessert,', 'snack,', 'chef,', 'kitchen', 'drinks,', 'alcohol,', 'party', 'space,', 'explore,', 'astronaut,', 'launch,', 'flight,', 'travel,', 'plane,', 'pilot,', 'sky', 'winning,', 'award,', 'achievement', 'entertainment,', 'acting,', 'movie', 'hospital,', 'sickness,', 'injury,', 'doctor,', 'medicine,', 'emergency', 'fire,', 'firefighters,', '911', 'home,', 'house,', 'living,', 'family,', 'domestic']\n"
     ]
    }
   ],
   "source": [
    "#create a list object with keywords that we will count the occurence of as a pretext task goal   \n",
    "\n",
    "individual_words = []\n",
    "\n",
    "for text in cleaned_list:\n",
    "    words= text.split()\n",
    "    individual_words.extend(words)\n",
    "    \n",
    "    \n",
    "print(individual_words)\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4aad3f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['funny', 'laughter', 'joke', 'comedian', 'sad', 'upset', 'crying', 'hurt', 'happy', 'joy', 'smile', 'thinking', 'question', 'learning', 'curious', 'mystery', 'student', 'learning', 'school', 'celebration', 'party', 'birthday', 'scared', 'worried', 'concerned', 'angry', 'mad', 'upset', 'hatred', 'frustrated', 'shocked', 'OMG', 'surprise', 'exasperated', 'computer', 'technology', 'social', 'media', 'money', 'job', 'stocks', 'sleepy', 'tired', 'sleep', 'bed', 'night', 'gross', 'disgusting', 'throw', 'up', 'puke', 'queasy', 'dead', 'deathly', 'poison', 'fossil', 'afterlife', 'good', 'great', 'ok', 'agreed', 'bad', 'no', 'disagree', 'empowerment', 'movement', 'teamwork', 'agreement', 'workout', 'sports', 'athlete', 'gym', 'muscle', 'religion', 'prayer', 'praise', 'beg', 'beauty', 'cosmetics', 'fashion', 'baby', 'kid', 'child', 'young', 'man', 'woman', 'elderly', 'old', 'love', 'relationship', 'significant', 'other', 'marriage', 'construction', 'building', 'ocean', 'sea', 'ship', 'water', 'justice', 'law', 'government', 'politics', 'pet', 'animal', 'vet', 'nature', 'plants', 'farming', 'science', 'earth', 'recycle', 'go', 'green', 'music', 'musician', 'instruments', 'artist', 'song', 'weather', 'rain', 'water', 'food', 'dessert', 'snack', 'chef', 'kitchen', 'drinks', 'alcohol', 'party', 'space', 'explore', 'astronaut', 'launch', 'flight', 'travel', 'plane', 'pilot', 'sky', 'winning', 'award', 'achievement', 'entertainment', 'acting', 'movie', 'hospital', 'sickness', 'injury', 'doctor', 'medicine', 'emergency', 'fire', 'firefighters', '911', 'home', 'house', 'living', 'family', 'domestic']\n"
     ]
    }
   ],
   "source": [
    "#comma taken out of list and each word it's own value \n",
    "\n",
    "character_to_remove=','\n",
    "cleaned_individual_words = [''.join(char for char in string if char not in character_to_remove)for string in individual_words]\n",
    "\n",
    "print(cleaned_individual_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f55a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretask goal: Label the instance for each keyword in the list \"cleaned_individual_words\" amongst 1000 text entry examples from cnn_dailynews\n",
    "\n",
    "#create empty list to store dataframe\n",
    "#articles_labels_df=pd.DataFrame()\n",
    "\n",
    "#Loop through list of keywords\n",
    "#Check if keyword is present amongst all rows for datapoint under article and if so label this as 1 and if not label 0\n",
    "#create dataframe for each individual keyword and their occurence amongst all samples \n",
    "#add individual dataframes to list articles_dataframe \n",
    "#concatenate articles_dataframe with articles_labels_df so all individual dataframes are together in one dataframe serving as columns \n",
    "\n",
    "##\n",
    "#for keyword in cleaned_individual_words:\n",
    " #   articles_keyword=pretask_dataset_tokenized['article_tokenized'].str.lower().str.contains(keyword.lower()).astype(int)\n",
    "    \n",
    "  #  articles_keyword=articles_keyword.rename('article_' + keyword)\n",
    "   \n",
    "   # articles_labels_df[articles_keyword.name] = articles_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "739c42af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#articles_labels_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75495c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n",
      "C:\\Users\\tdcap\\AppData\\Local\\Temp\\ipykernel_15932\\1538026596.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  highlights_labels_df[highlights_keyword.name] = highlights_keyword\n"
     ]
    }
   ],
   "source": [
    "#Now we must count occurences of keyword in summary column of our dataframe and then join these two dataframes together\n",
    "#together these will be the labels of our dataset that is text article entries and their summaries so a total of two features \n",
    "\n",
    "#So now use 'highlights' column and count occurences of keywords saved above to make a new column consisting of keyword +'highlights'\n",
    "\n",
    "highlights_labels_df=pd.DataFrame()\n",
    "\n",
    "for keyword in cleaned_individual_words:\n",
    "    highlights_keyword = pretask_dataset_tokenized['highlights_tokenized'].str.lower().str.contains(keyword.lower()).astype(int)\n",
    "    \n",
    "    highlights_keyword=highlights_keyword.rename('highlights_' + keyword)\n",
    "    \n",
    "    highlights_labels_df[highlights_keyword.name] = highlights_keyword    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "835f9a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NEW',\n",
       " ':',\n",
       " 'ĠThe',\n",
       " 'ĠUnited',\n",
       " 'ĠStates',\n",
       " 'Ġis',\n",
       " 'Ġoutraged',\n",
       " 'Ġby',\n",
       " 'Ġthe',\n",
       " 'Ġattack',\n",
       " ',',\n",
       " 'ĠSecretary',\n",
       " 'Ġof',\n",
       " 'ĠState',\n",
       " 'ĠRice',\n",
       " 'Ġsays',\n",
       " 'Ġ.',\n",
       " 'Ċ',\n",
       " 'Car',\n",
       " 'Ġbomb',\n",
       " 'Ġstrikes',\n",
       " 'ĠU',\n",
       " '.',\n",
       " 'S',\n",
       " '.',\n",
       " 'ĠEmbassy',\n",
       " 'Ġvehicle',\n",
       " 'Ġnorth',\n",
       " 'Ġof',\n",
       " 'ĠBeirut',\n",
       " 'Ġ.',\n",
       " 'Ċ',\n",
       " 'Three',\n",
       " 'ĠLebanese',\n",
       " 'Ġcivilians',\n",
       " 'Ġdead',\n",
       " ',',\n",
       " 'ĠAmerican',\n",
       " 'Ġand',\n",
       " 'ĠLebanese',\n",
       " 'Ġofficials',\n",
       " 'Ġconfirm',\n",
       " 'Ġ.',\n",
       " 'Ċ',\n",
       " 'Driver',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'Ġvehicle',\n",
       " 'Ġwas',\n",
       " 'Ġslightly',\n",
       " 'Ġinjured',\n",
       " ',',\n",
       " 'Ġand',\n",
       " 'Ġthe',\n",
       " 'Ġonly',\n",
       " 'Ġpassenger',\n",
       " 'Ġwas',\n",
       " 'Ġnot',\n",
       " 'Ġhurt',\n",
       " 'Ġ.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CHECKPOINT: Use row 198 to check if highlights datapoint for that row contains the word 'hurt'\n",
    "pretask_dataset.iloc[198, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02f0046f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with a value of 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>highlights_funny</th>\n",
       "      <th>highlights_laughter</th>\n",
       "      <th>highlights_joke</th>\n",
       "      <th>highlights_comedian</th>\n",
       "      <th>highlights_sad</th>\n",
       "      <th>highlights_upset</th>\n",
       "      <th>highlights_crying</th>\n",
       "      <th>highlights_hurt</th>\n",
       "      <th>highlights_happy</th>\n",
       "      <th>highlights_joy</th>\n",
       "      <th>...</th>\n",
       "      <th>highlights_medicine</th>\n",
       "      <th>highlights_emergency</th>\n",
       "      <th>highlights_fire</th>\n",
       "      <th>highlights_firefighters</th>\n",
       "      <th>highlights_911</th>\n",
       "      <th>highlights_home</th>\n",
       "      <th>highlights_house</th>\n",
       "      <th>highlights_living</th>\n",
       "      <th>highlights_family</th>\n",
       "      <th>highlights_domestic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 155 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     highlights_funny  highlights_laughter  highlights_joke  \\\n",
       "198                 0                    0                0   \n",
       "317                 0                    0                0   \n",
       "453                 0                    0                0   \n",
       "650                 0                    0                0   \n",
       "729                 0                    0                0   \n",
       "880                 0                    0                0   \n",
       "\n",
       "     highlights_comedian  highlights_sad  highlights_upset  highlights_crying  \\\n",
       "198                    0               0                 0                  0   \n",
       "317                    0               0                 0                  0   \n",
       "453                    0               0                 0                  0   \n",
       "650                    0               0                 0                  0   \n",
       "729                    0               0                 0                  0   \n",
       "880                    0               0                 0                  0   \n",
       "\n",
       "     highlights_hurt  highlights_happy  highlights_joy  ...  \\\n",
       "198                1                 0               0  ...   \n",
       "317                1                 0               0  ...   \n",
       "453                1                 0               0  ...   \n",
       "650                1                 0               0  ...   \n",
       "729                1                 0               0  ...   \n",
       "880                1                 0               0  ...   \n",
       "\n",
       "     highlights_medicine  highlights_emergency  highlights_fire  \\\n",
       "198                    0                     0                0   \n",
       "317                    0                     0                0   \n",
       "453                    0                     0                0   \n",
       "650                    0                     0                0   \n",
       "729                    0                     0                0   \n",
       "880                    0                     0                0   \n",
       "\n",
       "     highlights_firefighters  highlights_911  highlights_home  \\\n",
       "198                        0               0                0   \n",
       "317                        0               0                0   \n",
       "453                        0               0                0   \n",
       "650                        0               0                0   \n",
       "729                        0               0                0   \n",
       "880                        0               0                0   \n",
       "\n",
       "     highlights_house  highlights_living  highlights_family  \\\n",
       "198                 0                  0                  0   \n",
       "317                 0                  0                  0   \n",
       "453                 0                  0                  0   \n",
       "650                 0                  0                  0   \n",
       "729                 0                  0                  0   \n",
       "880                 0                  0                  0   \n",
       "\n",
       "     highlights_domestic  \n",
       "198                    0  \n",
       "317                    0  \n",
       "453                    0  \n",
       "650                    0  \n",
       "729                    0  \n",
       "880                    0  \n",
       "\n",
       "[6 rows x 155 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use mask to retreive rows which have the woord 'hurt' present in their highlights datapoint \n",
    "\n",
    "mask=highlights_labels_df['highlights_hurt']==1\n",
    "\n",
    "examples_with_hurt_highlights=highlights_labels_df[mask]\n",
    "\n",
    "#We see row 198 does have the keyword 'hurt' present so counting keyword occurences was successful\n",
    "print(\"Rows with a value of 1\")\n",
    "examples_with_hurt_highlights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d68c4671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "#Download transformer using \"namespace/modelname\" and trust_remote_code=True to allow downloading remote software \n",
    "\n",
    "\n",
    "\n",
    "#Use class name AutoModelForCausualLM \n",
    "\n",
    "\n",
    "\n",
    "phi_model= AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", from_tf=False, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47143512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>highlights_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Harry', 'ĠPotter', 'Ġstar', 'ĠDaniel', 'ĠRad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                highlights_tokenized\n",
       "0  ['Harry', 'ĠPotter', 'Ġstar', 'ĠDaniel', 'ĠRad..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretask_dataset_tokenized.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e71da24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#articles_labels_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb81cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df= pd.merge(pretask_dataset_tokenized, highlights_labels_df, left_index=True, right_index=True)\n",
    "\n",
    "#new_df_2=pd.merge(new_df, highlights_labels_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "977e431e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[17816, 18308,  3256,  ..., 50256, 50256, 50256],\n",
      "        [17816,    44,  3256,  ..., 50256, 50256, 50256],\n",
      "        [17816, 13965,  3256,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [17816, 29571,  3256,  ..., 50256, 50256, 50256],\n",
      "        [17816, 29511,  3256,  ..., 50256, 50256, 50256],\n",
      "        [17816, 10364,  3256,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs=new_df['highlights_tokenized'].tolist()\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "inputs = tokenizer(raw_inputs,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "407afdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harry', 'ĠPotter', 'Ġstar', 'ĠDaniel', 'ĠRad', 'cliffe', 'Ġgets', 'ĠÂ£', '20', 'M', 'Ġfortune', 'Ġas', 'Ġhe', 'Ġturns', 'Ġ18', 'ĠMonday', 'Ġ.', 'Ċ', 'Young', 'Ġactor', 'Ġsays', 'Ġhe', 'Ġhas', 'Ġno', 'Ġplans', 'Ġto', 'Ġf', 'rit', 'ter', 'Ġhis', 'Ġcash', 'Ġaway', 'Ġ.', 'Ċ', 'Rad', 'cliffe', \"'s\", 'Ġearnings', 'Ġfrom', 'Ġfirst', 'Ġfive', 'ĠPotter', 'Ġfilms', 'Ġhave', 'Ġbeen', 'Ġheld', 'Ġin', 'Ġtrust', 'Ġfund', 'Ġ.']<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "#view tokenized input example\n",
    "print(tokenizer.decode(inputs['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d864e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2a56fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Currently storing only labels after dropping tokenized text column\n",
    "#Educatioal Code Cell\n",
    "tokenized_dataset = Dataset.from_pandas(new_df.drop(columns='highlights_tokenized', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9200022e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Educatioal Code Cell\n",
    "#In order to access datapoint of huggingface compatible dataset you must index like a dictionary \n",
    "#so index using column name which is a list value for features key\n",
    "#Index again into the row you want to retrieve the example of \n",
    "\n",
    "tokenized_dataset['highlights_funny'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcdd7234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Value\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e99545f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store binary labels from new_df in new variable named 'labels_df' by dropping only column that doesn't count labels\n",
    "#highlights_tokenized column contains summary of article text that is tokenized\n",
    "\n",
    "labels_df=new_df.drop(columns='highlights_tokenized', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de117c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all values from labels_df and store into object variable named binary_labels\n",
    "binary_labels=labels_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48b94b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make binary_labels a tensor object of dtype torch.float32 to be compatible with making tensor dataset \n",
    "tensor_labels=torch.tensor(binary_labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb67f62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tensor_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7472620f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6875287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15efa073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create tensor dataset now using TensorDataset and passing each tensor object through \n",
    "\n",
    "dataset= TensorDataset(inputs['input_ids'], inputs['attention_mask'], tensor_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c697eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make tensordataset named 'dataset' a huggingface compatible dataset by loading dataset using Dataset.from_dict\n",
    "#Index into dataset starting from [0] then [1] then [2] which is the same order of how we made dataset above \n",
    "#Indexing like the above sentence into dataset retrieves the corresponding key values for the key names mentioned in the next comment\n",
    "#Each key for dictionary needs to be in hugging face format so 'input_ids' & 'attention_mask' & 'labels' must be key names\n",
    "#model recognizes appropriate input ids which are text examples that are tokenized \n",
    "#model recognizes appropriate attention_mask which is used to batch input sequence together \n",
    "#Convert each value returned from indexing into dataset as list to be compatible value for training process as\n",
    "#training requires indexing into dictionary using key to retrieve value that IS NOT A DICTIONARY TYPE OBJECT \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hf_dataset = Dataset.from_dict({\n",
    "    'input_ids': dataset.tensors[0].tolist(),\n",
    "    'attention_mask': dataset.tensors[1].tolist(),\n",
    "    'labels': dataset.tensors[2].tolist(),\n",
    "})\n",
    "\n",
    "#hf naming convention to say this object/variable with 'hf' in the name of the dataset is huggingface compatible now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "592b99a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format dataset to be compatible with pytorch\n",
    "hf_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c54b0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use default training arguments and assign to training_args to be used in Trainer instance \n",
    "training_args=TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58b9229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset into training and test splits so hf_split_dataset contains a train dataset of 900 examples and a test dataset of 100 examples\n",
    "\n",
    "hf_split_dataset=hf_dataset.train_test_split(test_size=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36d456e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make two objects, the first variable 'pretask_train' to store the training subset from the split \n",
    "#The latter variable 'pretask_eval' to store the test subset from the split\n",
    "pretask_train=hf_split_dataset['train']\n",
    "pretask_eval=hf_split_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ecd9ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle new subsets from previous cell and select 'integer' within range of both subsets in previous cell \n",
    "#This takes even smaller subset of data to finetune and evaluate on in order to speed up training process \n",
    "\n",
    "small_train_dataset=pretask_train.shuffle(seed=42).select(range(90))\n",
    "small_eval_dataset=pretask_eval.shuffle(seed=42).select(range(90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a51cdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "#load microsoft phi-1.5 transformer and specify 'num_labels' parameter it should expect which we found length of highlights_labels_df\n",
    "#length of highlights_labels_df which contains all binary labels for highlights_tokenied has 155 columns \n",
    "#specify 155 for 'num_labels' parameter in 'from_pretrained' method \n",
    "\n",
    "\n",
    "phi_model= AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", from_tf=False, trust_remote_code=True, num_labels=155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a919ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a3e3fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment this out for now this is in the case I would like to manually train \n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8215096",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=3\n",
    "optimizer=AdamW(phi_model.parameters(), lr= 2e-4)\n",
    "num_training_steps=num_epochs * len(train_dataloader)\n",
    "lr_scheduler=get_scheduler(\n",
    " name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d220ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=Trainer(\n",
    "    model=phi_model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9751b35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixFormerSequentialForCausalLM(\n",
       "  (layers): Sequential(\n",
       "    (0): Embedding(\n",
       "      (wte): Embedding(51200, 2048)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (2): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (3): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (4): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (5): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (6): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (7): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (8): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (9): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (10): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (11): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (12): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (13): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (14): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (15): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (16): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (17): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (18): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (19): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (20): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (21): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (22): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (23): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (24): ParallelBlock(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mixer): MHA(\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        (inner_attn): SelfAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (inner_cross_attn): CrossAttention(\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "      )\n",
       "    )\n",
       "    (25): CausalLMHead(\n",
       "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Linear(in_features=2048, out_features=51200, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss): CausalLMLoss(\n",
       "    (loss_fct): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "phi_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e635d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tdcap\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 90\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 36\n",
      "  Number of trainable parameters = 1418270720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`attention_mask` is not supported during training. Using it might lead to unexpected results.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bf584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now we must evaluate model performance on small evaluation dataset \n",
    "#Once finetuning/training is done phi_model is finetuned to understand keywords we want it to pay attention to in input sequence it processes \n",
    "#Now it's ready to generate emojis for it's main task by counting keyword presence in examples \n",
    "#then using the presence of certain key words to generate an emoji corresponding with the summary of the text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "388f4ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#In the case with need to use the optimizer \n",
    "from torch.optim import AdamW"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
